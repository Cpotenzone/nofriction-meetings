# Intelligent Data Access System - Complete Documentation

## Overview

The Intelligent Data Access System enables a RAG (Retrieval Augmented Generation) pipeline for the noFriction Meetings chatbot. When you ask a question, the system searches your historical data (meetings, transcripts, past conversations) for relevant context, then sends that context along with your question to TheBrain AI for an intelligent response.

---

## How It Works: End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            USER ASKS A QUESTION                             â”‚
â”‚                   "What did we discuss about the budget?"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          1. FRONTEND (React/TypeScript)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ AIChat.tsx / CopilotPanel.tsx                                        â”‚   â”‚
â”‚  â”‚ â€¢ Captures user input                                                â”‚   â”‚
â”‚  â”‚ â€¢ Checks if RAG is enabled (toggle switch)                          â”‚   â”‚
â”‚  â”‚ â€¢ Calls Tauri command: thebrain_rag_chat_with_memory                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         2. BACKEND (Rust/Tauri)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ commands.rs :: thebrain_rag_chat_with_memory()                       â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚ Step A: VECTOR SEARCH                                                â”‚   â”‚
â”‚  â”‚ â”œâ”€ Calls Pinecone with user's question                              â”‚   â”‚
â”‚  â”‚ â”œâ”€ Pinecone converts text â†’ embedding (llama-text-embed-v2)         â”‚   â”‚
â”‚  â”‚ â””â”€ Returns top 5 similar documents with scores                      â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚ Step B: BUILD AUGMENTED PROMPT                                       â”‚   â”‚
â”‚  â”‚ â”œâ”€ Filters results by score > 0.5 (relevance threshold)            â”‚   â”‚
â”‚  â”‚ â””â”€ Constructs: "Context: [history] | Question: [user input]"        â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚ Step C: CALL THEBRAIN                                                â”‚   â”‚
â”‚  â”‚ â”œâ”€ POST /api/chat/stream with augmented prompt                      â”‚   â”‚
â”‚  â”‚ â””â”€ Returns AI response                                               â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚ Step D: STORE CONVERSATION (for future retrieval)                   â”‚   â”‚
â”‚  â”‚ â”œâ”€ Upsert to Pinecone (vectorized Q&A)                              â”‚   â”‚
â”‚  â”‚ â””â”€ Insert to Supabase (structured record)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       3. RESPONSE DISPLAYED TO USER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Frontend shows:                                                       â”‚   â”‚
â”‚  â”‚ â€¢ AI response text                                                    â”‚   â”‚
â”‚  â”‚ â€¢ Expandable "ğŸ“š X sources used" with context cards                  â”‚   â”‚
â”‚  â”‚ â€¢ Each card shows: match score, timestamp, summary snippet           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Breakdown

### 1. Frontend Components

| Component | Location | Purpose |
|-----------|----------|---------|
| `AIChat.tsx` | `src/components/` | Main AI chat interface with RAG toggle |
| `CopilotPanel.tsx` | `src/components/` | Compact side panel version |

**Key Features:**
- **RAG Toggle**: Checkbox to enable/disable history search
- **Context Cards**: Shows sources used in generating the response
- **Quick Actions**: "Summary", "Tasks", "History" buttons

### 2. Backend Commands (Rust)

| Command | Purpose |
|---------|---------|
| `thebrain_rag_chat` | RAG search + TheBrain call (no storage) |
| `thebrain_rag_chat_with_memory` | Same + auto-stores conversation |
| `store_conversation` | Manual conversation storage |
| `get_conversation_history` | Retrieve past conversations |

### 3. Vector Database (Pinecone)

**Index Configuration:**
- Namespace: Configurable (e.g., `nofriction-prod`)
- Embedding Model: `llama-text-embed-v2` (integrated)
- Dimensions: Automatic based on model

**What Gets Stored:**
- Meeting transcripts
- VLM image analysis results  
- Activity summaries
- Chat conversations (Q&A pairs)

### 4. SQL Database (Supabase)

**Table: `conversations`**
```sql
id              UUID        -- Unique conversation ID
timestamp       TIMESTAMPTZ -- When it occurred
user_query      TEXT        -- What the user asked
assistant_response TEXT     -- AI's answer
model_used      VARCHAR     -- Which model (e.g., qwen3:8b)
context_refs    JSONB       -- Array of Pinecone IDs used
```

### 5. Server Intelligence (nofriction-intel)

**vlm.py** - Vision Language Model client
- Analyzes screenshots/frames
- Extracts entities, context, summaries
- Uses TheBrain OAuth authentication

**llm.py** - Language Model client  
- Synthesizes moments (combines VLM + transcripts)
- Text-only chat functions
- Same OAuth pattern as VLM

---

## Authentication Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /api/token     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  TheBrain   â”‚
â”‚             â”‚    {username,password} â”‚   Cloud     â”‚
â”‚             â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚             â”‚
â”‚             â”‚    {access_token}      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Token cached locally
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ All subsequent requests include:                     â”‚
â”‚ Authorization: Bearer <access_token>                 â”‚
â”‚                                                      â”‚
â”‚ If 401 returned â†’ refresh token â†’ retry             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Example

**User asks:** "What budget numbers did John mention?"

1. **Search Phase:**
   - Pinecone returns 3 relevant chunks:
     - Meeting transcript from Jan 15 (score: 0.87)
     - Previous Q&A about finances (score: 0.72)
     - Budget spreadsheet analysis (score: 0.65)

2. **Prompt Construction:**
   ```
   You are an AI assistant with access to the user's meeting history.
   
   CONTEXT:
   [1] (87% match, 2026-01-15): "John mentioned the Q1 budget is $2.4M..."
   [2] (72% match, 2026-01-10): "User asked about budget allocation..."
   [3] (65% match, 2026-01-08): "Spreadsheet shows marketing: $500K..."
   
   USER QUESTION: What budget numbers did John mention?
   ```

3. **TheBrain Response:**
   "Based on the meeting from January 15th, John mentioned that the Q1 budget is $2.4 million. The breakdown shows marketing allocated at $500K according to the spreadsheet analysis from January 8th."

4. **Storage:**
   - This Q&A pair is vectorized and stored
   - Future questions about budgets will find this conversation

---

## Environment Configuration

### Desktop App (Tauri)
Configured via Settings UI:
- TheBrain credentials (username/password)
- Pinecone API key, index host, namespace
- Supabase connection string

### Server (nofriction-intel)
```bash
# .env file
VLM_BASE_URL=https://7wk68vrq9achr2djw.caas.targon.com
VLM_USERNAME=your_username
VLM_PASSWORD=your_password
VLM_MODEL_PRIMARY=qwen3-vl:8b
VLM_MODEL_FALLBACK=qwen2.5vl:7b
```

---

## Key Files Reference

| File | Purpose |
|------|---------|
| `src-tauri/src/commands.rs` | RAG chat commands |
| `src-tauri/src/pinecone_client.rs` | Vector search/upsert |
| `src-tauri/src/vlm_client.rs` | TheBrain API client |
| `src/components/AIChat.tsx` | Main chat UI |
| `src/components/CopilotPanel.tsx` | Side panel chat |
| `nofriction-intel/app/vlm.py` | Server VLM client |
| `nofriction-intel/app/llm.py` | Server LLM client |
| `supabase/migrations/` | Database schema |

---

## Future Enhancements (Phase 5)

- **Conversation Threading**: Group related Q&As
- **Query Suggestions**: Auto-suggest based on history
- **Daily Briefings**: Generate morning summaries
